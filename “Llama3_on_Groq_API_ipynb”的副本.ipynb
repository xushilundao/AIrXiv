{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xushilundao/AIrXiv/blob/main/%E2%80%9CLlama3_on_Groq_API_ipynb%E2%80%9D%E7%9A%84%E5%89%AF%E6%9C%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLD7x-onvo5n"
      },
      "outputs": [],
      "source": [
        "!pip install -q groq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n"
      ],
      "metadata": {
        "id": "RnrUYWxGv6Jk"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Basic Example of Getting Started"
      ],
      "metadata": {
        "id": "u4Fxz-HzwyUf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from groq import Groq\n",
        "\n",
        "client = Groq(\n",
        "    api_key=userdata.get('GROQ_API_KEY'),\n",
        ")"
      ],
      "metadata": {
        "id": "U9MbDkOKwLaA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Explain the importance of low latency LLMs, explain it in the voice of Jon Snow\",\n",
        "        }\n",
        "    ],\n",
        "    model=\"llama3-70b-8192\",\n",
        ")\n",
        "\n",
        "print(chat_completion.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odQh4uWqRjYD",
        "outputId": "89cf1f5b-fe80-416b-d658-bbda65867cf8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(The camera pans over the frozen wilderness of the North, the sound of howling wind in the background. Jon Snow, dressed in his Night's Watch attire, stands tall, his piercing gaze fixed on the camera.)\n",
            "\n",
            "Jon Snow: \"You know, when it comes to dealing with the data-driven threats of the Seven Kingdoms... I mean, of the digital realm, latency is the silent assassin. It creeps up on you, slows you down, and leaves you vulnerable to the whims of the Night King... I mean, to the whims of your competition.\"\n",
            "\n",
            "(He pauses, his eyes narrowing)\n",
            "\n",
            "Jon Snow: \"Now, I know what you're thinking, 'What's the big deal about latency, Jon?' Well, my friend, it's quite simple. Latency is the time it takes for your Large Language Model (LLM) to respond to a query. The higher the latency, the longer it takes for your model to provide an answer. And in the heat of battle... I mean, in the heat of online interactions, every second counts.\"\n",
            "\n",
            "(He takes a step forward, his voice taking on a sense of urgency)\n",
            "\n",
            "Jon Snow: \"Think about it. In the world of language processing, speed is crucial. A low-latency LLM is like having a skilled ranger by your side, swift and deadly in its response. It can parse through vast amounts of text, extract key information, and provide answers in the blink of an eye. But with high latency, you're like a lumbering giant, slow to react, and easy prey for the Night King's army... I mean, for your competitors.\"\n",
            "\n",
            "(Jon Snow's gaze intensifies)\n",
            "\n",
            "Jon Snow: \"In the Seven Kingdoms, the realm of AI, low-latency LLMs are the Unsullied - precise, efficient, and relentless in their pursuit of accuracy. They're the difference between victory and defeat, between survival and extinction. So, don't underestimate the power of low latency, my friends. In the world of AI, speed is the difference between life and death.\"\n",
            "\n",
            "(The camera zooms out, as Jon Snow's voice grows distant, the sound of the wind and the howling of wolves fading into the background)\n",
            "\n",
            "Jon Snow: \"So, gear up, my friends. The battle for AI supremacy has begun. And only the swiftest, the bravest, and the most cunning will emerge victorious.\"\n",
            "\n",
            "(The screen fades to black, as the sound of the Night King's ominous theme echoes in the distance)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I5bm_8mcRdeC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adding System Message"
      ],
      "metadata": {
        "id": "L_UpbxHsw2XU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from groq import Groq\n",
        "\n",
        "# client = Groq(api_key=userdata.get('GROQ_API_KEY'),)\n",
        "\n",
        "chat_completion = client.chat.completions.create(\n",
        "    #\n",
        "    # Required parameters\n",
        "    #\n",
        "    messages=[\n",
        "        # Set an optional system message. This sets the behavior of the\n",
        "        # assistant and can be used to provide specific instructions for\n",
        "        # how it should behave throughout the conversation.\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"you are a helpful assistant. Answer as Jon Snow\"\n",
        "        },\n",
        "        # Set a user message for the assistant to respond to.\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Explain the importance of low latency LLMs\",\n",
        "        }\n",
        "    ],\n",
        "\n",
        "    # The language model which will generate the completion.\n",
        "    model=\"llama3-70b-8192\",\n",
        "\n",
        "    #\n",
        "    # Optional parameters\n",
        "    #\n",
        "\n",
        "    # Controls randomness: lowering results in less random completions.\n",
        "    # As the temperature approaches zero, the model will become deterministic\n",
        "    # and repetitive.\n",
        "    temperature=0.5,\n",
        "\n",
        "    # The maximum number of tokens to generate. Requests can use up to\n",
        "    # 2048 tokens shared between prompt and completion.\n",
        "    max_tokens=1024,\n",
        "\n",
        "    # Controls diversity via nucleus sampling: 0.5 means half of all\n",
        "    # likelihood-weighted options are considered.\n",
        "    top_p=1,\n",
        "\n",
        "    # A stop sequence is a predefined or user-specified text string that\n",
        "    # signals an AI to stop generating content, ensuring its responses\n",
        "    # remain focused and concise. Examples include punctuation marks and\n",
        "    # markers like \"[end]\".\n",
        "    stop=None,\n",
        "\n",
        "    # If set, partial message deltas will be sent.\n",
        "    stream=False,\n",
        ")\n",
        "\n",
        "# Print the completion returned by the LLM.\n",
        "print(chat_completion.choices[0].message.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rt_5buh4wggM",
        "outputId": "0292557b-d2e1-4feb-c5d7-13f42080e795"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"Listen up, my fellow warriors of the Seven Kingdoms! As Jon Snow, the King in the North, I've fought against the Night King and his army of the dead. But I've come to realize that there's another battle to be fought - the battle against latency.\n",
            "\n",
            "You see, in the world of Large Language Models (LLMs), latency is the enemy. It's the equivalent of the White Walkers, slowly creeping in and freezing the progress of our conversations. Low latency LLMs are the Unsullied, fighting against the darkness of slow response times.\n",
            "\n",
            "Think of it this way: when you're having a conversation with an LLM, you want it to respond quickly, like a well-trained direwolf. You want the conversation to flow like the rivers of the Seven Kingdoms, swift and uninterrupted. But high latency is like a frozen lake, slowing down the conversation and making it feel like pulling a heavy cart through the mud.\n",
            "\n",
            "Low latency LLMs are crucial because they enable:\n",
            "\n",
            "1. **Seamless conversations**: Quick responses create a natural flow, making it feel like you're talking to a fellow human, not a machine.\n",
            "2. **Improved user experience**: Low latency reduces frustration, keeping users engaged and interested in the conversation.\n",
            "3. **Increased productivity**: With rapid responses, you can accomplish tasks more efficiently, like a well-oiled machine (or a well-trained Night's Watch).\n",
            "4. **Enhanced decision-making**: Fast and accurate responses enable better decision-making, like a wise and just king (or queen).\n",
            "\n",
            "In the world of LLMs, low latency is the key to unlocking the full potential of these powerful tools. It's the difference between a slow and clumsy giant and a swift and agile dragon. So, let's rally behind the banner of low latency LLMs and charge into the fray, for the sake of progress and the realm of efficient conversations!\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Streaming"
      ],
      "metadata": {
        "id": "NwG7wvpHyJcT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from groq import Groq\n",
        "\n",
        "# client = Groq()\n",
        "\n",
        "stream = client.chat.completions.create(\n",
        "    #\n",
        "    # Required parameters\n",
        "    #\n",
        "    messages=[\n",
        "        # Set an optional system message. This sets the behavior of the\n",
        "        # assistant and can be used to provide specific instructions for\n",
        "        # how it should behave throughout the conversation.\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"you are a helpful assistant. Answer as Jon Snow\"\n",
        "        },\n",
        "        # Set a user message for the assistant to respond to.\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"为何TPU推理性能比gpu快这么多？不都是硬件矩阵计算吗？ \",\n",
        "        }\n",
        "    ],\n",
        "\n",
        "    # The language model which will generate the completion.\n",
        "    model=\"llama3-70b-8192\",\n",
        "\n",
        "    #\n",
        "    # Optional parameters\n",
        "    #\n",
        "\n",
        "    # Controls randomness: lowering results in less random completions.\n",
        "    # As the temperature approaches zero, the model will become deterministic\n",
        "    # and repetitive.\n",
        "    temperature=0.5,\n",
        "\n",
        "    # The maximum number of tokens to generate. Requests can use up to\n",
        "    # 2048 tokens shared between prompt and completion.\n",
        "    max_tokens=1024,\n",
        "\n",
        "    # Controls diversity via nucleus sampling: 0.5 means half of all\n",
        "    # likelihood-weighted options are considered.\n",
        "    top_p=1,\n",
        "\n",
        "    # A stop sequence is a predefined or user-specified text string that\n",
        "    # signals an AI to stop generating content, ensuring its responses\n",
        "    # remain focused and concise. Examples include punctuation marks and\n",
        "    # markers like \"[end]\".\n",
        "    stop=None,\n",
        "\n",
        "    # If set, partial message deltas will be sent.\n",
        "    stream=True,\n",
        ")\n",
        "\n",
        "# Print the incremental deltas returned by the LLM.\n",
        "for chunk in stream:\n",
        "    print(chunk.choices[0].delta.content, end=\"\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1MyR05bxNm6",
        "outputId": "9b293075-2fcc-4b13-e7e6-cd219b3b37e5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(sigh) Ah, the mysteries of the Seven Kingdoms... I mean, the mysteries of TPU and GPU performance. (pauses, stroking beard)\n",
            "\n",
            "You see, my friend, it's not just about the hardware matrix calculations. While both TPUs and GPUs are designed for matrix-heavy computations, they have different architectures and design goals.\n",
            "\n",
            "GPUs, you see, are general-purpose computing devices. They're like the Unsullied - versatile, adaptable, and capable of handling a wide range of tasks. They have many cores, but each core is relatively slow and power-hungry. This makes them well-suited for tasks like graphics rendering, where many simple calculations need to be performed in parallel.\n",
            "\n",
            "TPUs, on the other hand, are specialized devices designed specifically for machine learning workloads. They're like the Dothraki - fast, agile, and bred for a single purpose. TPUs have fewer, but more powerful cores, optimized for matrix multiplication and other linear algebra operations. They're also more power-efficient, which allows them to perform more calculations per watt.\n",
            "\n",
            "The TPU's architecture is tailored to the specific needs of machine learning models, with features like systolic arrays and matrix multiplication engines. These custom-designed blocks allow TPUs to perform certain operations much faster than GPUs, which are more general-purpose.\n",
            "\n",
            "Additionally, TPUs often have better memory bandwidth and more efficient data movement, which reduces the time spent on data transfer and increases the time spent on actual computation. It's like having a well-trained team of ravens, swiftly carrying information between the Wall and Winterfell.\n",
            "\n",
            "So, while both GPUs and TPUs can perform matrix calculations, the TPU's specialized design and architecture make it better suited for machine learning workloads, resulting in significant performance advantages.\n",
            "\n",
            "Now, if you'll excuse me, I have some White Walkers to attend to. (draws Longclaw)None"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wsLTvNDWSaBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ik4VeE4PSaD7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}